{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from   matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "from   scipy.stats import norm\n",
    "from scipy.stats import anderson\n",
    "from math import log2\n",
    "from math import sqrt\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get annotations for each time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('\\\\TCPDBench\\\\analysis\\\\annotations\\\\annotations.json') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load each time series and standarize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASETS CAN BE FOUND HERE: https://github.com/alan-turing-institute/TCPDBench\n",
    "\n",
    "DATASETS = [\n",
    "#    \"apple\",\n",
    "    \"bank\",\n",
    "#    \"bee_waggle_6\",\n",
    "    \"bitcoin\",\n",
    "    \"brent_spot\",\n",
    "    \"businv\",\n",
    "    \"centralia\",\n",
    "    \"children_per_woman\",\n",
    "    \"co2_canada\",\n",
    "    \"construction\",\n",
    "    \"debt_ireland\",\n",
    "    \"gdp_argentina\",\n",
    "    \"gdp_croatia\",\n",
    "    \"gdp_iran\",\n",
    "    \"gdp_japan\",\n",
    "    \"global_co2\",\n",
    "    \"homeruns\",\n",
    "#    \"iceland_tourism\",\n",
    "    \"jfk_passengers\",\n",
    "    \"lga_passengers\",\n",
    "    \"measles\",\n",
    "    \"nile\",\n",
    "#    \"occupancy\",\n",
    "    \"ozone\",\n",
    "    \"quality_control_1\",\n",
    "    \"quality_control_2\",\n",
    "    \"quality_control_3\",\n",
    "    \"quality_control_4\",\n",
    "    \"quality_control_5\",\n",
    "    \"rail_lines\",\n",
    "    \"ratner_stock\",\n",
    "    \"robocalls\",\n",
    "#    \"run_log\",\n",
    "    \"scanline_126007\",\n",
    "    \"scanline_42049\",\n",
    "    \"seatbelts\",\n",
    "    \"shanghai_license\",\n",
    "    \"uk_coal_employ\",\n",
    "    \"unemployment_nl\",\n",
    "    \"usd_isk\",\n",
    "    \"us_population\",\n",
    "    \"well_log\",\n",
    "]\n",
    "DATASET_NAMES = {k: k for k in DATASETS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeriesData = []\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    filename = '..\\\\TCPD\\\\datasets\\\\{}\\\\'.format(dataset)\n",
    "    with open(filename+'{}.json'.format(dataset)) as f:\n",
    "        timeSeriesData.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdTestingData = []\n",
    "\n",
    "for i in range(len(timeSeriesData)):\n",
    "    rawData = timeSeriesData[i]['series'][0]['raw']\n",
    "    if None in rawData:\n",
    "        indexOfNone = [u for u,v in enumerate(rawData) if v == None]\n",
    "        for iN in indexOfNone:\n",
    "            rawData[iN] = (rawData[iN-1] + rawData[iN+1]) / 2\n",
    "\n",
    "    rawData = np.asarray(rawData)\n",
    "    rawData = (rawData - np.mean(rawData)) / np.std(rawData)\n",
    "    stdTestingData.append(rawData)\n",
    "    print(len(stdTestingData[i]))\n",
    "    if(len(stdTestingData[i])< 40):\n",
    "        print(DATASETS[i])\n",
    "\n",
    "stdTestingData = np.asarray(stdTestingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Covering and F1-Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(T, X, margin=5):\n",
    "    \"\"\"Compute true positives without double counting\n",
    "\n",
    "    >>> true_positives({1, 10, 20, 23}, {3, 8, 20})\n",
    "    {1, 10, 20}\n",
    "    >>> true_positives({1, 10, 20, 23}, {1, 3, 8, 20})\n",
    "    {1, 10, 20}\n",
    "    >>> true_positives({1, 10, 20, 23}, {1, 3, 5, 8, 20})\n",
    "    {1, 10, 20}\n",
    "    >>> true_positives(set(), {1, 2, 3})\n",
    "    set()\n",
    "    >>> true_positives({1, 2, 3}, set())\n",
    "    set()\n",
    "    \"\"\"\n",
    "    # make a copy so we don't affect the caller\n",
    "    X = set(list(X))\n",
    "    TP = set()\n",
    "    for tau in T:\n",
    "        close = [(abs(tau - x), x) for x in X if abs(tau - x) <= margin]\n",
    "        close.sort()\n",
    "        if not close:\n",
    "            continue\n",
    "        dist, xstar = close[0]\n",
    "        TP.add(tau)\n",
    "        X.remove(xstar)\n",
    "    return TP\n",
    "\n",
    "\n",
    "def f_measure(annotations, predictions, margin=5, alpha=0.5, return_PR=False):\n",
    "    \"\"\"Compute the F-measure based on human annotations.\n",
    "\n",
    "    annotations : dict from user_id to iterable of CP locations\n",
    "    predictions : iterable of predicted CP locations\n",
    "    alpha : value for the F-measure, alpha=0.5 gives the F1-measure\n",
    "    return_PR : whether to return precision and recall too\n",
    "\n",
    "    Remember that all CP locations are 0-based!\n",
    "\n",
    "    >>> f_measure({1: [10, 20], 2: [11, 20], 3: [10], 4: [0, 5]}, [10, 20])\n",
    "    1.0\n",
    "    >>> f_measure({1: [], 2: [10], 3: [50]}, [10])\n",
    "    0.9090909090909091\n",
    "    >>> f_measure({1: [], 2: [10], 3: [50]}, [])\n",
    "    0.8\n",
    "    \"\"\"\n",
    "    # ensure 0 is in all the sets\n",
    "    Tks = {k + 1: set(annotations[uid]) for k, uid in enumerate(annotations)}\n",
    "    for Tk in Tks.values():\n",
    "        Tk.add(0)\n",
    "\n",
    "    X = set(predictions)\n",
    "    X.add(0)\n",
    "\n",
    "    Tstar = set()\n",
    "    for Tk in Tks.values():\n",
    "        for tau in Tk:\n",
    "            Tstar.add(tau)\n",
    "\n",
    "    K = len(Tks)\n",
    "\n",
    "    P = len(true_positives(Tstar, X, margin=margin)) / len(X)\n",
    "\n",
    "    TPk = {k: true_positives(Tks[k], X, margin=margin) for k in Tks}\n",
    "    R = 1 / K * sum(len(TPk[k]) / len(Tks[k]) for k in Tks)\n",
    "\n",
    "    F = P * R / (alpha * R + (1 - alpha) * P)\n",
    "    if return_PR:\n",
    "        return F, P, R\n",
    "    return F\n",
    "\n",
    "\n",
    "def overlap(A, B):\n",
    "    \"\"\" Return the overlap (i.e. Jaccard index) of two sets\n",
    "\n",
    "    >>> overlap({1, 2, 3}, set())\n",
    "    0.0\n",
    "    >>> overlap({1, 2, 3}, {2, 5})\n",
    "    0.25\n",
    "    >>> overlap(set(), {1, 2, 3})\n",
    "    0.0\n",
    "    >>> overlap({1, 2, 3}, {1, 2, 3})\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    return len(A.intersection(B)) / len(A.union(B))\n",
    "\n",
    "\n",
    "def partition_from_cps(locations, n_obs):\n",
    "    \"\"\" Return a list of sets that give a partition of the set [0, T-1], as \n",
    "    defined by the change point locations.\n",
    "\n",
    "    >>> partition_from_cps([], 5)\n",
    "    [{0, 1, 2, 3, 4}]\n",
    "    >>> partition_from_cps([3, 5], 8)\n",
    "    [{0, 1, 2}, {3, 4}, {5, 6, 7}]\n",
    "    >>> partition_from_cps([1,2,7], 8)\n",
    "    [{0}, {1}, {2, 3, 4, 5, 6}, {7}]\n",
    "    >>> partition_from_cps([0, 4], 6)\n",
    "    [{0, 1, 2, 3}, {4, 5}]\n",
    "    \"\"\"\n",
    "    T = n_obs\n",
    "    partition = []\n",
    "    current = set()\n",
    "\n",
    "    all_cps = iter(sorted(set(locations)))\n",
    "    cp = next(all_cps, None)\n",
    "    for i in range(T):\n",
    "        if i == cp:\n",
    "            if current:\n",
    "                partition.append(current)\n",
    "            current = set()\n",
    "            cp = next(all_cps, None)\n",
    "        current.add(i)\n",
    "    partition.append(current)\n",
    "    return partition\n",
    "\n",
    "\n",
    "def cover_single(Sprime, S):\n",
    "    \"\"\"Compute the covering of a segmentation S by a segmentation Sprime.\n",
    "\n",
    "    This follows equation (8) in Arbaleaz, 2010.\n",
    "\n",
    "    >>> cover_single([{1, 2, 3}, {4, 5}, {6}], [{1, 2, 3}, {4, 5, 6}])\n",
    "    0.8333333333333334\n",
    "    >>> cover_single([{1, 2, 3, 4}, {5, 6}], [{1, 2, 3, 4, 5, 6}])\n",
    "    0.6666666666666666\n",
    "    >>> cover_single([{1, 2}, {3, 4}, {5, 6}], [{1, 2, 3}, {4, 5, 6}])\n",
    "    0.6666666666666666\n",
    "    >>> cover_single([{1, 2, 3, 4, 5, 6}], [{1}, {2}, {3}, {4, 5, 6}])\n",
    "    0.3333333333333333\n",
    "    \"\"\"\n",
    "    T = sum(map(len, Sprime))\n",
    "    assert T == sum(map(len, S))\n",
    "    C = 0\n",
    "    for R in S:\n",
    "        C += len(R) * max(overlap(R, Rprime) for Rprime in Sprime)\n",
    "    C /= T\n",
    "    return C\n",
    "\n",
    "\n",
    "def Covering(annotations, predictions, n_obs):\n",
    "    \"\"\"Compute the average segmentation covering against the human annotations.\n",
    "\n",
    "    annotations : dict from user_id to iterable of CP locations\n",
    "    predictions : iterable of predicted Cp locations\n",
    "    n_obs : number of observations in the series\n",
    "\n",
    "    >>> covering({1: [10, 20], 2: [10], 3: [0, 5]}, [10, 20], 45)\n",
    "    0.7962962962962963\n",
    "    >>> covering({1: [], 2: [10], 3: [40]}, [10], 45)\n",
    "    0.7954144620811286\n",
    "    >>> covering({1: [], 2: [10], 3: [40]}, [], 45)\n",
    "    0.8189300411522634\n",
    "\n",
    "    \"\"\"\n",
    "    Ak = {\n",
    "        k + 1: partition_from_cps(annotations[uid], n_obs)\n",
    "        for k, uid in enumerate(annotations)\n",
    "    }\n",
    "    pX = partition_from_cps(predictions, n_obs)\n",
    "\n",
    "    Cs = [cover_single(pX, Ak[k]) for k in Ak]\n",
    "    return sum(Cs) / len(Cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SB-BOCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianOnlineChangePointDetection_Segment:\n",
    "    def __init__(self, hazard, distribution):\n",
    "        self.hazard = hazard\n",
    "        self.distribution = distribution\n",
    "        self.T = 0\n",
    "        self.beliefs = np.zeros((1, 2),dtype=np.float64)\n",
    "        self.beliefs[0, 0] = 1.0\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.T = 0\n",
    "        self.beliefs = np.zeros((1, 2))\n",
    "        self.beliefs[0, 0] = 1.0\n",
    "\n",
    "    def _expand_belief_matrix(self):\n",
    "        rows = np.zeros((1, 2),dtype=np.float64)\n",
    "        self.beliefs = np.concatenate((self.beliefs, rows), axis=0)\n",
    "\n",
    "    def _shift_belief_matrix(self):\n",
    "        self.beliefs[:, 0] = self.beliefs[:, 1]\n",
    "        self.beliefs[:, 1] = 0.0\n",
    "\n",
    "    def update(self, x, studentTQueue, historicalData):\n",
    "        \n",
    "        \n",
    "        # Evaluate Predictive Probability (3 in Algortihm 1)\n",
    "        if type(x) is not np.ndarray:\n",
    "            self._expand_belief_matrix()\n",
    "\n",
    "            self.distribution.update_params(x)\n",
    "            # Update internal state\n",
    "            #print(len(self.beliefs))\n",
    "\n",
    "            self.T += 1\n",
    "        else:\n",
    "            self._expand_belief_matrix()\n",
    "            \n",
    "\n",
    "\n",
    "            pi_t = self.distribution.pdf(x, studentTQueue, historicalData)\n",
    "            # Calculate H(r_{t-1})\n",
    "            h = self.hazard(self.rt)\n",
    "\n",
    "            self.beliefs[1 : self.T + 2, 1] = self.beliefs[: self.T + 1, 0] * pi_t * (1 - h)\n",
    "\n",
    "            # Calculate Changepoint Probabilities (5 in Algorithm 1)\n",
    "            self.beliefs[0, 1] = (self.beliefs[: self.T + 1, 0] * pi_t * h).sum()\n",
    "\n",
    "            # Determine Run length Distribution (7 in Algorithm 1)\n",
    "            self.beliefs[:, 1] = self.beliefs[:, 1] / self.beliefs[:, 1].sum()\n",
    "\n",
    "            # Update sufficient statistics (8 in Algorithm 8)\n",
    "            self.distribution.update_params(x[0])\n",
    "\n",
    "            # Update internal state\n",
    "            self._shift_belief_matrix()\n",
    "            self.T += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def rt(self):\n",
    "       # return np.where(self.beliefs[:, 0] == self.beliefs[:, 0].max())[0]\n",
    "    \n",
    "        rtPick = np.where(self.beliefs[:, 0] > 0.5)[0]\n",
    "        #print(rtPick)\n",
    "        if(len(rtPick) == 0):\n",
    "            return 0\n",
    "        else:\n",
    "            return rtPick[-1]\n",
    "    @property\n",
    "    def belief(self):\n",
    "        return self.beliefs[:, 0]\n",
    "\n",
    "class Hazard:\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ConstantHazard(Hazard):\n",
    "    def __init__(self, _lambda):\n",
    "        self._lambda = _lambda\n",
    "\n",
    "    def __call__(self, r):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          r: The length of the current run (np.ndarray or scalar)\n",
    "\n",
    "        Returns:\n",
    "          p: Changepoint Probabilities(np.ndarray with shape = r.shape)\n",
    "        \"\"\"\n",
    "        if isinstance(r, np.ndarray):\n",
    "            shape = r.shape\n",
    "        else:\n",
    "            shape = 1\n",
    "\n",
    "        return np.ones(shape) / self._lambda\n",
    "\n",
    "class Distribution:\n",
    "    def reset_params(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def pdf(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_params(self, x):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class StudentT(Distribution):\n",
    "    \"\"\" Generalized Student t distribution \n",
    "    https://en.wikipedia.org/wiki/Student%27s_t-distribution#Generalized_Student's_t-distribution\n",
    "\n",
    "    This setting corresponds to select\n",
    "      1: Gaussian distribution as a likelihood\n",
    "      2: normal-Gamma distribution as a prior for Gaussian\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu=0, kappa=1, alpha=1, beta=1):\n",
    "        self.mu0 = np.array([mu],dtype=np.float64)\n",
    "        self.kappa0 = np.array([kappa],dtype=np.float64)\n",
    "        self.alpha0 = np.array([alpha],dtype=np.float64)\n",
    "        self.beta0 = np.array([beta],dtype=np.float64)\n",
    "        # We need the following lines to prevent \"outside defined warning\"\n",
    "        self.muT = self.mu0.copy()\n",
    "        self.kappaT = self.kappa0.copy()\n",
    "        self.alphaT = self.alpha0.copy()\n",
    "        self.betaT = self.beta0.copy()\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.muT = self.mu0.copy()\n",
    "        self.kappaT = self.kappa0.copy()\n",
    "        self.alphaT = self.alpha0.copy()\n",
    "        self.betaT = self.beta0.copy()\n",
    "\n",
    "    def pdf(self, x, studentTQueue, historicalData):\n",
    "        \"\"\" Probability Density Function\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        return stats.t.pdf(\n",
    "            np.average(x),\n",
    "            loc=self.muT,\n",
    "            df=2 * self.alphaT,\n",
    "            scale=np.sqrt(self.betaT * (self.kappaT + 1) / (self.alphaT * self.kappaT)),\n",
    "        )\n",
    "\n",
    "    def update_params(self, x):\n",
    "        \"\"\"Update Sufficient Statistcs (Parameters)\n",
    "\n",
    "        To understand why we use this, see e.g.\n",
    "        Conjugate Bayesian analysis of the Gaussian distribution, Kevin P. Murphy∗\n",
    "        https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf\n",
    "        3.5 Posterior predictive\n",
    "        \"\"\"\n",
    "        self.betaT = np.concatenate(\n",
    "            [\n",
    "                self.beta0,\n",
    "                (self.kappaT + (self.kappaT * (x - self.muT) ** 2) / (2 * (self.kappaT + 1))),\n",
    "            ]\n",
    "        )\n",
    "        self.muT = np.concatenate([self.mu0, (self.kappaT * self.muT + x) / (self.kappaT + 1)])\n",
    "        self.kappaT = np.concatenate([self.kappa0, self.kappaT + 1])\n",
    "        self.alphaT = np.concatenate([self.alpha0, self.alphaT + 0.5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SB-BOCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "fMeasures = np.zeros(len(DATASETS))\n",
    "coverings = np.zeros(len(DATASETS))\n",
    "\n",
    "\n",
    "alpha = [0.01,1,100]\n",
    "beta = [0.01,1,100]\n",
    "kappa = [0.01,1,100]\n",
    "hazard = [50,100,200]\n",
    "segmentLength = [3,5,8,13,21,34]\n",
    "\n",
    "for a in alpha:\n",
    "    for b in beta:\n",
    "        for k in kappa:\n",
    "            for l in segmentLength:\n",
    "                for n,datasetName in enumerate(DATASETS):\n",
    "                    studentTQueue = []\n",
    "\n",
    "                    bc = BayesianOnlineChangePointDetection_Segment(ConstantHazard(300), StudentT(mu=0, kappa=k, alpha=a, beta=b))\n",
    "                    \n",
    "                    \n",
    "                    testSignal=stdTestingData[11]\n",
    "                    historicalData = []\n",
    "                    rt_mle = np.empty(testSignal.shape)\n",
    "                    L = l\n",
    "\n",
    "                    cuttoff = math.floor(len(testSignal)/50)\n",
    "                    allData = []\n",
    "\n",
    "                    # Online estimation and get the maximum likelihood r_t at each time point\n",
    "                    for i, d in enumerate(testSignal):\n",
    "                        #print(d)\n",
    "                        #print(testSignal[0:20])\n",
    "                        allData.append(d)\n",
    "                        if i >= L:\n",
    "                            for j in range(len(historicalData)):\n",
    "                                historicalData[j].append(testSignal[i-L])\n",
    "                            historicalData.insert(0,[testSignal[i-L]])\n",
    "                            localData = np.asarray(allData[-L:])\n",
    "                            \n",
    "                            bc.update(localData,studentTQueue,historicalData)\n",
    "                            rt_mle[i] = bc.rt\n",
    "                        else: \n",
    "                            rt_mle[i] = 0\n",
    "\n",
    "                    plt.plot(rt_mle)\n",
    "                    plt.show()\n",
    "                    # Plot data with estimated change points\n",
    "                    plt.plot(testSignal, alpha=0.5, label=\"observation\")\n",
    "                    \n",
    "                    index_changes = np.where(np.diff(rt_mle)<-cuttoff)[0]-int(L/2)\n",
    "                    if(f_measure(annotations[datasetName], index_changes) == 1.0):\n",
    "                        print(l)\n",
    "                        print('********************************')\n",
    "                        continue\n",
    "                    print(index_changes)\n",
    "                    plt.scatter(index_changes, testSignal[index_changes], c='green', label=\"change point\")\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print('alpha' + str(a))\n",
    "                    print('beta' + str(b))\n",
    "                    print('kappa' + str(k))\n",
    "                    \n",
    "                    print(datasetName)\n",
    "                    \n",
    "                    print(\"F-Measure\")\n",
    "                    print(f_measure(annotations[datasetName], index_changes))\n",
    "                    \n",
    "                    fMeasure = f_measure(annotations[datasetName], index_changes)\n",
    "                    if fMeasure > fMeasures[n]:\n",
    "                        fMeasures[n] = fMeasure\n",
    "                   \n",
    "                    print(\"Covering\")\n",
    "                    covering = Covering(annotations[datasetName], index_changes,len(testSignal))\n",
    "                    print(covering)\n",
    "                    if covering > coverings[n]:\n",
    "                        coverings[n] = covering\n",
    "                        \n",
    "for c in coverings:\n",
    "    print(c)\n",
    "print()\n",
    "for c in fMeasures:\n",
    "    print(c)                      \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
